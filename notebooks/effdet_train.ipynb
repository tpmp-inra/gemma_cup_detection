{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientDet *Marchantia polymorpha* Gemma Cup Detection\n",
    "\n",
    "https://medium.com/data-science-at-microsoft/training-efficientdet-on-custom-data-with-pytorch-lightning-using-an-efficientnetv2-backbone-1cdf3bd7921f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import loaders as lds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"..\").joinpath(\"data_in\")\n",
    "images_path = data_path.joinpath(\"images\")\n",
    "dataset_path = Path(\"..\").joinpath(\"data_in\", \"datasets\")\n",
    "\n",
    "data_path.is_dir(), images_path.is_dir(), dataset_path.is_dir()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"..\").joinpath(\"data_in\")\n",
    "dataset_path = Path(\"..\").joinpath(\"data_in\", \"datasets\")\n",
    "\n",
    "train = pd.read_csv(dataset_path.joinpath(\"train.csv\"))\n",
    "val = pd.read_csv(dataset_path.joinpath(\"val.csv\"))\n",
    "test = pd.read_csv(dataset_path.joinpath(\"test.csv\"))\n",
    "\n",
    "train.shape, val.shape, test.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_size = 512\n",
    "tst_ds = lds.GemmaDataset(\n",
    "    train,\n",
    "    images_path=images_path,\n",
    "    transform=lds.get_test_image_transform(image_size=image_size),\n",
    "    bboxes=True,\n",
    "    return_id=True,\n",
    "    yxyx=True,\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    tst_ds.draw_image_with_boxes(filename=train.sample(n=1).filename.to_list()[0])\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = train.sample(n=1).filename.to_list()[0]\n",
    "\n",
    "lds.make_patches_grid(\n",
    "    images=[tst_ds.draw_image_with_boxes(filename=file_name) for _ in range(12)],\n",
    "    row_count=3,\n",
    "    col_count=4,\n",
    "    figsize=(10, 7.5),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class GemmaCupDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataset_adaptor,\n",
    "        validation_dataset_adaptor,\n",
    "        train_transforms=lds.get_train_transform(image_size=image_size),\n",
    "        valid_transforms=lds.get_valid_transform(image_size=image_size),\n",
    "        num_workers=0,\n",
    "        batch_size=8,\n",
    "    ):\n",
    "        self.train_ds = train_dataset_adaptor\n",
    "        self.valid_ds = validation_dataset_adaptor\n",
    "        self.train_tfms = train_transforms\n",
    "        self.valid_tfms = valid_transforms\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        super().__init__()\n",
    "\n",
    "    def train_dataset(self) -> lds.GemmaDataset:\n",
    "        return lds.GemmaDataset(\n",
    "            self.train_ds,\n",
    "            transform=self.train_tfms,\n",
    "            images_path=images_path,\n",
    "            bboxes=True,\n",
    "            return_id=True,\n",
    "            yxyx=True,\n",
    "        )\n",
    "\n",
    "    def val_dataset(self) -> lds.GemmaDataset:\n",
    "        return lds.GemmaDataset(\n",
    "            self.valid_ds,\n",
    "            transform=self.valid_tfms,\n",
    "            images_path=images_path,\n",
    "            bboxes=True,\n",
    "            return_id=True,\n",
    "            yxyx=True,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        train_dataset = self.train_dataset()\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        valid_dataset = self.val_dataset()\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return valid_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, targets, image_ids = tuple(zip(*batch))\n",
    "        images = torch.stack(images)\n",
    "        images = images.float()\n",
    "\n",
    "        boxes = [target[\"bboxes\"].float() for target in targets]\n",
    "        labels = [target[\"labels\"].float() for target in targets]\n",
    "        img_size = torch.tensor([target[\"img_size\"] for target in targets]).float()\n",
    "        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).float()\n",
    "\n",
    "        annotations = {\n",
    "            \"bbox\": boxes,\n",
    "            \"cls\": labels,\n",
    "            \"img_size\": img_size,\n",
    "            \"img_scale\": img_scale,\n",
    "        }\n",
    "\n",
    "        return images, annotations, targets, image_ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet.config.model_config import efficientdet_model_param_dict\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "from effdet.config.model_config import efficientdet_model_param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of configs: {len(efficientdet_model_param_dict)}')\n",
    "\n",
    "list(efficientdet_model_param_dict.keys())[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models('s*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=1, image_size=512, architecture=\"tf_efficientnetv2_s\"):\n",
    "    efficientdet_model_param_dict[architecture] = dict(\n",
    "        name=architecture,\n",
    "        backbone_name=architecture,\n",
    "        backbone_args=dict(drop_path_rate=0.2),\n",
    "        num_classes=num_classes,\n",
    "        url=\"\",\n",
    "    )\n",
    "\n",
    "    config = get_efficientdet_config(architecture)\n",
    "    config.update({\"num_classes\": num_classes})\n",
    "    config.update({\"image_size\": (image_size, image_size)})\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "    )\n",
    "    return DetBenchTrain(net, config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "class GemmaCupEfficientDetModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=1,\n",
    "        img_size=512,\n",
    "        prediction_confidence_threshold=0.2,\n",
    "        learning_rate=0.0002,\n",
    "        wbf_iou_threshold=0.44,\n",
    "        inference_transforms=lds.get_valid_transform(image_size=image_size),\n",
    "        model_architecture=\"tf_efficientnetv2_l\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = create_model(\n",
    "            num_classes, img_size, architecture=model_architecture\n",
    "        )\n",
    "        self.prediction_confidence_threshold = prediction_confidence_threshold\n",
    "        self.lr = learning_rate\n",
    "        self.wbf_iou_threshold = wbf_iou_threshold\n",
    "        self.inference_tfms = inference_transforms\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, annotations, _, image_ids = batch\n",
    "\n",
    "        losses = self.model(images, annotations)\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": losses[\"class_loss\"].detach(),\n",
    "            \"box_loss\": losses[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            losses[\"loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_class_loss\",\n",
    "            losses[\"class_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_box_loss\",\n",
    "            losses[\"box_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return losses[\"loss\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, annotations, targets, image_ids = batch\n",
    "        outputs = self.model(images, annotations)\n",
    "\n",
    "        detections = outputs[\"detections\"]\n",
    "\n",
    "        batch_predictions = {\n",
    "            \"predictions\": detections,\n",
    "            \"targets\": targets,\n",
    "            \"image_ids\": image_ids,\n",
    "        }\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": outputs[\"class_loss\"].detach(),\n",
    "            \"box_loss\": outputs[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\n",
    "            \"valid_loss\",\n",
    "            outputs[\"loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"valid_class_loss\",\n",
    "            logging_losses[\"class_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"valid_box_loss\",\n",
    "            logging_losses[\"box_loss\"],\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        return {\"loss\": outputs[\"loss\"], \"batch_predictions\": batch_predictions}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = GemmaCupDataModule(\n",
    "    train_dataset_adaptor=train,\n",
    "    validation_dataset_adaptor=val,\n",
    "    num_workers=0,\n",
    "    batch_size=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GemmaCupEfficientDetModel(num_classes=1, img_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=5,\n",
    "    num_sanity_val_steps=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1188155435dbec4475398bbe2d3efc9cfb02b2400b681277646382219e71d1cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
