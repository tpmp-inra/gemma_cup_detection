{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfactory-breathing",
   "metadata": {},
   "source": [
    "# Gemma cup detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-friday",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import transforms as T\n",
    "\n",
    "from skimage import io, transform\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-airfare",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"data_in\")\n",
    "images_path =os.path.join(data_path, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-cream",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-medline",
   "metadata": {},
   "source": [
    "### Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-better",
   "metadata": {},
   "source": [
    "### Create class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "basic-discussion",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36db2ada6db4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mGemmaDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GemmaDataset(Dataset):\n",
    "    def __init__(self, csv, transform=None):\n",
    "        self.boxes = csv.copy()\n",
    "        self.images = list(self.boxes.filename.unique())\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.boxes.shape[0]\n",
    "    \n",
    "    def load_boxes(self, idx):\n",
    "        boxes = self.boxes[self.boxes.filename == self.images[idx]].dropna()\n",
    "        size = boxes.shape[0]\n",
    "        if size > 0:\n",
    "            boxes = boxes[['x', 'y', 'width', 'height']].values\n",
    "            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "            return size, boxes\n",
    "        return 0, []\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        return rgb2gray(\n",
    "            io.imread(\n",
    "                os.path.join(\n",
    "                    images_path, \n",
    "                    self.images[idx]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        num_box, boxes = self.load_boxes(index) # return list of [xmin, ymin, xmax, ymax]\n",
    "        img = self.load_image(index) # return an image\n",
    "\n",
    "        if num_box > 0:\n",
    "          boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        else:\n",
    "          #negative example, ref: https://github.com/pytorch/vision/issues/2144\n",
    "          boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        image_id = torch.tensor([index])\n",
    "        target = {\n",
    "            \"boxes\":boxes,\n",
    "            \"labels\":torch.ones((num_box,), dtype=torch.int64),\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": torch.as_tensor(\n",
    "                (boxes[:, 3] - boxes[:, 1])*(boxes[:, 2] - boxes[:, 0]), \n",
    "                dtype=torch.float32\n",
    "            ),\n",
    "            \"iscrowd\": torch.zeros((num_box,), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "        \n",
    "        return img, target, image_id        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-large",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GemmaDataset(pd.read_csv(os.path.join(data_path, \"boxes.csv\")))\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-tribune",
   "metadata": {},
   "source": [
    "#### Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.images[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-front",
   "metadata": {},
   "source": [
    "#### Test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.boxes.sort_values([\"filename\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.boxes[ds.boxes.filename == ds.images[test_index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-establishment",
   "metadata": {},
   "source": [
    "#### Test boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds.load_boxes(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_boxes(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_boxes(test_index)[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-chapter",
   "metadata": {},
   "source": [
    "#### Test load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ds.load_image(test_index)\n",
    "io.imshow(img) \n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-prophet",
   "metadata": {},
   "source": [
    "#### Test item selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[test_index][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-phrase",
   "metadata": {},
   "source": [
    "## RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasterRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# Define RPN \n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=tuple([(16, 32, 64, 128, 256) for _ in range(5)]), # let num of tuple equal to num of feature maps\n",
    "    aspect_ratios=tuple([(0.75, 0.5, 1.25) for _ in range(5)]),  # ref: https://github.com/pytorch/vision/issues/978\n",
    ")\n",
    "\n",
    "rpn_head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "\n",
    "fasterRCNN.rpn = RegionProposalNetwork(\n",
    "    anchor_generator= anchor_generator, head= rpn_head,\n",
    "    fg_iou_thresh= 0.7, bg_iou_thresh=0.3,\n",
    "    batch_size_per_image=48, # use fewer proposals\n",
    "    positive_fraction = 0.5,\n",
    "    pre_nms_top_n=dict(training=200, testing=100),\n",
    "    post_nms_top_n=dict(training=160, testing=80),\n",
    "    nms_thresh = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-season",
   "metadata": {},
   "source": [
    "## Fast R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = fasterRCNN.roi_heads.box_predictor.cls_score.in_features #get number of features\n",
    "fasterRCNN.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = 2)\n",
    "fasterRCNN.roi_heads.fg_bg_sampler.batch_size_per_image = 24\n",
    "fasterRCNN.roi_heads.fg_bg_sampler.positive_fraction = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-lesson",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-trance",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes: tuple = (0.8, 0.20)\n",
    "batch_size = 10\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"boxes.csv\"))\n",
    "\n",
    "dataset_size = len(df)\n",
    "indices = [i for i in df.index]\n",
    "\n",
    "split_train = int(np.floor(sizes[0] * dataset_size))\n",
    "split_test = int(np.floor(sizes[1] * dataset_size)) + split_train\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = (\n",
    "    indices[:split_train],\n",
    "    indices[split_train:split_test],\n",
    ")\n",
    "\n",
    "df_train = df.iloc[train_indices]\n",
    "df_test = df.iloc[test_indices]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    GemmaDataset(csv=df_train, transform=get_transform(train=True)),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    GemmaDataset(csv=df_test, transform=get_transform(train=False)),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-compiler",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# move model to the right device\n",
    "fasterRCNN.to(device)\n",
    "\n",
    "params = [p for p in fasterRCNN.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(\n",
    "    params, \n",
    "    lr=0.0005, \n",
    "    betas=(0.9, 0.999), \n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=3, \n",
    "    gamma=0.1\n",
    ")\n",
    "metric_collector = []\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 5 iterations\n",
    "    metric_logger = train_one_epoch(\n",
    "        fasterRCNN, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        device, \n",
    "        epoch, \n",
    "        print_freq=5\n",
    "    )\n",
    "    metric_collector.append(metric_logger)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # Evaluate with validation dataset\n",
    "    metric_logger_val = validate(fasterRCNN, val_data_loader, device, print_freq=5)\n",
    "    #save checlpoint\n",
    "    torch.save(\n",
    "        fasterRCNN.state_dict(), \n",
    "        os.path.join( weights_path,'fasterRCNN_ep'+str(epoch)+'.pth')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-american",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
